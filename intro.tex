\chapter{Introduction}

Memory in its different forms is an important aspect of human, but also animal cognition.
For example, it allows animals to return to previously visited water and food locations~\parencite{vorhees2014}.
Prior experiences also allow to act more optimally in similar situations or to avoid dangerous situations.
In this sense, memory allows for adaptation on a faster time scale than genetic selection.
This is especially important in unstable and changing environments.
In humans, memory is also important to form social relationships, a shared culture, and even a functioning society.
For example, strategies in the repeated prisoner's dilemma depend on working memory capacity~\parencite{milinski1998}.
Moreover, memory contributes to our individual sense of self~\parencite{prebble2013}.
Apart from this functional perspective, an addressable memory is also important from a computational perspective.
It allows for a more compact implementation of many computational processes than a pure state machine could achieve~\parencite{gallistel2009}.
Overall, the storage of information over time is a fundamental requirement in many cognitive systems.

Accordingly, there is a long history of memory research.
The well-known primacy and recency effect (discussed in more detail in \cref{sec:exp-findings}) have already been described by \textcite{Robinson1926}.
Nevertheless, there are still many open questions.
One important challenge that memory systems have to solve is the so-called \emph{stability-plasticity dilemma}~\parencite{Abraham2005}.
On the one hand, there is a need to quickly form new memories, sometimes even with a single exposure known as \emph{one-shot learning}.
On the other hand, such high plasticity can easily lead to overwriting of old memories rendering memory system useless.
\Textcite{Buzsaki1989} and others proposed multi-stage memory models where different memory systems are in place for different timescales with different levels of plasticity.

Despite this, much of experimental research and modeling treat different memory systems as isolated.
This simplifies the analysis on a certain level, but to get a general understanding of memory as a whole the results need to integrated at some point.
Furthermore, many models of memory or learning focus on either small scale neural changes without any direct connection to behaviour or on the other extreme of describing behaviour with mathematical equations, but no solid grounding in biological plausibility.
So it is not only important to integrate our understanding of memory systems, but also to bridge the gap from neural mechanisms to behaviour.
With this thesis I advance the understanding in this direction by proposing a model integrating short- and long-term memory and thus modeling their interaction.
Moreover, I implement the model as a spiking neural network to ensure biological plausibility, while at the same time matching behavioural data.

While this will still be comparably rough sketch and still fairly limited in memory systems covered, there are promising long-term prospects from a better understanding of human memory.
Many forms of memory loss are currently untreatable.
However, diseases associated with aging, like Alzheimer's, significantly impair the function of memory and are getting more common as our life-span increases due to other medical and nutritional advances.
A better understanding of memory might allow us to devise better treatments or even stop and reverse the memory detoriation.
A potential route to this are memory implants that have already been demonstrated in rats by~\textcite{Berger2011}.
For these sort of implants, an understanding of how memories are encoded is at least helpful if not crucial.

These are certainly strong motivators for research into memory, but it is also worthwhile to advance our general understanding of how the human brain works.
A question that has puzzled researchers for a long time and probably still will for a long time to come despite great advances.
An important step in testing our current understanding of the brain was the Spaun model~\parencite{Eliasmith2012}.
A spiking neural network of 2.5 million neurons, thus grounded in biology, that can perform 8 different tasks.
Like a real brain it gets sensory input (low resolution black and white symbols) and produces a behavioural motor output with a simulated arm.
The number of tasks it can perform demonstrates that it is not a specialized model for a single task, but can switch between different tasks like a real brain.
Obviously, Spaun is still much simpler (and has much less neurons) than an actual brain and still far from a full understanding of the brain.
But it incorporates a number of qualitative key aspects such as the ability to switch task, making human-like errors, and being implemented in a spiking neural network.
However, one key aspect is still missing.
While it can perform working memory dependent tasks and has simple reinforcement learning, it is missing a declarative and episodic long-term memory.
The work in this thesis can also be seen as a step toward implementing a model of such memory for the future integration in even larger scale models integrating further key aspects of cognition.


\section{Behavioural characterization of memory}
Memory systems have been characterized in different and sometimes contradictory ways.
However, one commonly used distinction is made along two orthogonal dimensions:
by timescale and by type of information stored.
On the timescale axis one can differentiate between short-term memory (STM) lasting for up to tens of seconds and long-term memory (LTM) which can last between hours to decades~\parencite{chaudhuri2016}.
However, there is no single agreed upon definition of the exact boundaries.
Sometimes the term very long-term memory (vLTM) is used in addition to STM and LTM for memory that exceeds LTM (TODO ref).

Another use of these terms defines STM as memory being maintained through sustained neural firing, while LTM is realized by synaptic-weight changes.
The volatility of sustained neural firing explains why also in this use of terms, STM usually corresponds to information maintained on shorter timescales than in LTM\@.
In this classification, vLTM would correspond to information that has been consolidated from hippocampus to neo-cortical connections.
Moreover, the term working memory (WM) is often used to refer to active representations that allow direct mental manipulation.
In this thesis, I will use the latter definition of these terms.

When classifying memory by type of information, a representation as a tree structure can be helpful (\cref{fig:memtypes}).
On the highest level, we have the distinction between implicit and explicit memory.
Implicit memory does not allow for conscious access.
A typical example is procedural or motor memory like how to ride a bike.
Explicit memory allows for conscious access and is further subdivided into declarative and episodic memory.
The declarative memory allows us to store and reproduce facts like the birthday of a friend, whereas the episodic memory provides a recollection of life experiences.
\begin{figure}
    \centering
    \begin{tikzpicture}
        [level 1/.style={sibling distance=7cm},
        level 2/.style={sibling distance=2.5cm}]
        \node {Memory}
        child {node {\strut Explict} child {node {\strut Episodic}} child {node 
                {\strut Declarative}} child {node {\strut Semantic}}}
        child {node {\strut Implict} child {node {\strut Priming}} child {node 
                {\strut Conditioning}} child {node {\strut Procedural}}};
    \end{tikzpicture}
    \caption{Categorization of memory by type of stored 
        information.}\label{fig:memtypes}
\end{figure}

Here, I will focus on declarative memory as this type of memory has been well studied in many memory experiments.


\section{Experimental findings in memory research}\label{sec:exp-findings}
Most memory experiments require the participants to memorize lists of words and recall them afterwards.
Usually an experiment will fall in one of two categories depending on how the subjects are required to recall the list items: in free recall experiments, the subjects are free to recall the items in any order; in serial recall experiments, the subjects are required to recall the items in order.

The hallmark finding in memory research, especially for serial recall experiments, are the primacy and recency effect.
Subjects tend to remember the start of list (primacy) and the end of list (recency) better (\cref{fig:exp-serial-pos}).
Furthermore, if subjects in a serial recall experiment recall an item at the wrong position, a so-called \emph{transposition}, it is more likely to be an item that was close in the list than an item several positions spaced out.
\begin{figure}
    \centering
    \includegraphics{figures/exp-serial-pos}
    \caption[Immediate serial recall position curve]{Serial position curve of a immediate serial recall experiment with a 10 item list. Data reproduced from \textcite{Jahnke1968} with 95\% confidence intervals.}\label{fig:exp-serial-pos}
\end{figure}

In free recall experiments, a recency effect is observed as well.
This does not only show in the serial position curve, but participants often start out with recalling the last item first.
This can be measured by the probability of first recall (\cref{fig:exp-free-recall}).
Another important aspect of free recall is captured by the conditional response probability (CRP).
It gives the probability for the difference (the \emph{lag}) in serial positions of two recalled items.
It is peaked around 0, indicating that items in proximity in the learned list tend to be recalled together, while jumps to remote items are rarer.
This is known as \emph{contiguity} or the \emph{lag-recency effect}.
The CRP curve also has a characteristic asymmetry which shows the bias for forward (opposed to backward) recall.
\begin{figure}
    \centering
    \includegraphics{figures/exp-free-recall}
    \caption[Free recall probablitiy of first recall and CRP]{Data from free recall experiments with 12 item lists. (a) Probability of first recall. (b) Conditional response probability. All data from \textcite{Howard1999} with 95\% confidence intervals.}\label{fig:exp-free-recall}
\end{figure}

By introducing a delay filled with a distractor task, referred to as \emph{delayed (free) recall}, both the probability of first recall and the CRP curve will become much flatter.
This effect cannot solely be attributed to (suppressed) rehearsal effects as introducing an equally long distractor interval inbetween the list items, known as \emph{continuous distractor (free) recall}, partially restores the recency effect in the probability of first recall.

TODO scopolamine and hebb repetition.


\section{Neuroanatomy of memory}
TODO brain regions involved in short term recall

The hippocampus (HC) has been implicated in the acquisition of new episodic memories.
A finding derived from patients with hippocampal lesions, most famously HM who got his hippocampus removed due to severe epilepsy (TODO refs).
The hippocampus is named for his sea horse shaped structure.
It is located in the medial temporal lobe (TODO fig) and neighbours the subiculum and entorhinal cortex (EC).
The hippocampus is further divided into substructures CA3, CA1, and the dentate gyrus (DG) by its cytoarchitectural structure.
While the CA3 and CA1 regions consist mainly out of pyramidal neurons and about \SI{10}{\percent} GABAergic, inhibitory interneurons (TODO ref Freund and Buzsaki 1996), granule cells are the main constituent of the dentate gyrus.
Furthermore, the dentate gyrus has a large number of cells compared to CA3.
In humans the cell count of the DG is about twice as high as in CA3, whereas in rats DG has about five times more cells than the CA3 region.

The connectivity to, from, and within hippocampus is exceptionally well known.
There are two major pathways from entorhinal cortex.
The direct pathway originates in layer III of the entorhinal cortex and targets the distal apical dendrites of CA1 neurons.
The trisynaptic pathway originates in layer II and leads via the dentate gyrus, and CA3 also to CA1, but targeting the proximal dendrites (a signal thus crosses three synapses).
The connections from the dentate gyrus to the CA3 region in this is termed the mossy fiber pathway.
Each of these mossy fibers innervates about 15 neurons (TODO ref Claiborne, Amaral and Cowan 1986) which results in each CA3 pyramidal neuron receiving input from about 50 to 90 granule cells (TODO ref Squire, Shimamura, and Amaral 1989, p230).
The final bundle of connections from CA3 to CA1 is known as the Schaffer collateral pathway.

TODO further connections: EC to CA3 (mostly targeting same cells as from DG, Paxinos 2014), CA3 recurrent (single cell not innervated by more than 5\% of all CA3 cells Squire, Shimamura and Amaral 1989, p231)

Given these neuroanatomical properties, some of the hippocampal regions have been assumed to be involved in specific tasks.
The sparse cdoing and large cell count of the dentate gyrus led to the belief that it is responsible for pattern separation (TODO ref Rolls 2013).
The CA3 region with its recurrent connectivity could be involved in pattern completion or forward predictions (TODO ref Leutgeb and Leutgeb 2007, Rolls 2013).
Additional evidence for this comes from context dependent firing of these neurons (TODO how so? TODO ref Guzowski, KHierim, and Moser 2004, Leutgeb and Leutgeb 2007) and unimpaired recognition memory after lesioning the CA3 to CA1 connections despite impairing selective recall (TODO ref Brun et al 2002). 

theta rhythm, SPWs, grid and place cells

vLTM not hippocampal
Cerebellum for motor


\section{Memory models}
TODO discuss desired properties of models
TODO categorization by conceptual, abstract mathematical, and neural models

\subsection{Conceptual models}
\Textcite{Yntema1963} presented one of the first models of memory.
They conceptualized retrieval as a search process where each item in memory is associated with tags referencing further information.
For example, a time tag would encode the time an item was observed.
The whole description, however, is more based on how a memory system could be implemented on a classical von Neumann computer and does not consider if or how those operations could be neurally implemented.

To date, the most influential conceptual organization of working memory was proposed by \textcite{Baddeley1986}.
He proposed, based on experimental data, separate stores for visual and acoustic information, termed the visuospatial sketchpad and phonological loop respectively.
These are controlled by a central executive.
Furthermore, in \textcite{Baddeley2000} the model was extended with an episodic buffer for the binding of multimodal information and transfer to episodic long-term memory.
The main relevance of the model is that it informs us about the organization of (working) memory by modality; but it does less to elucidate mechanisms.

In general, conceptual models can give a high-level account and they can be evaluated with respect to their qualitative agreement to behavioural data.
However, they cannot provide us with quantitative predictions which makes a more rigorous validation difficult.
They also, do not explain the cognitive mechanisms in detail and an account of the neural implementation is completely out of their reach.


\subsection{Mathematical models}
Some of the weaknesses of conceptual models are addressed by mathematical models.
These models will make quantitative predictions about the behavioural data and describe underlying cognitive mechanisms to a varying degree, but do not provide a neural implementation.
There is a vast amount of such models and not all can be discussed here.
Thus I will focus on the most influential ones.

That list certainly contains the perturbation model for serial order by \textcite{Estes1972}, the free recall model Search of Associative Memory (SAM) by \textcite{Raaijmakers1981}, the recognition memory model Retrieving Effectively from Memory (REM) by \textcite{Shiffrin1997}, and the episodic memory model MINERVA2 by \textcite{Hintzman1988}.
The perturbation model an early attempt to provide a mathematical framework of how remembered item positions can drift over time.
In SAM cues are assembled in a short-term memory to retrieve associations from a long-term associative memory (the search part of the model).
In the REM model error prone copies of feature vectors derived from the study items are stored.
A recognition probe is matched to the stored feature vectors and a likelihood ratio of the match scores being generated by an old vs.\ a new item is calculated.
The MINERVA2 model also uses feature vectors that are stored as traces and can be probed by cues.

All of these models, and most other mathematical model, either assume item-to-item or position-to-item associations.
The primacy model by \textcite{Page1998} is worth noting because it uses a different approach.
In that model items are activated according to a primacy gradient, but the model is agnostic to how this gradient is generated.
This, however, also leaves that aspect underspecified.

Common to all of these models is that they are not demonstrating biological plausibility.
Thus it is unclear, whether any of the models can be implemented in a neural substrate while preserving the predictions.
Or even if, what the limitations with regard to noise and requirements for neural resources are.
Nevertheless, mathematical modelling is an important first step in figuring out what sort of processes are worth considering for a neural implementation.
Also, there are some reoccurring ideas in these models that are useful to consider in the context of a neural model.
For example, starting with \textcite{Anderson1973} many models have used random feature vectors to represent individual items.
That approach is similar to the Semantic Pointer Architecture (SPA) presented in Section TODO which can be implemented neurally with the methods of the Neural Engineering Framework (NEF, Section TODO).

An especially influential model based and random feature vectors was TODAM2 \parencite{Murdock1993}.
It was able to fit a large body of experimental data and in that aims to be a general theory for item recognition, serial order, and associative memory.
A neural implementation could very well be possible with the NEF, but has not been attempted so far.
However, \textcite{Choo2010} pointed out that the dimensionality of the vectors in the model increases with each stored item.
Thus, the requirement for neural resources grows unbounded.
It also worth noting, that TODAM2 does not give an account of how responses are generated.

Another useful idea, that had its origin in mathematical models, is the idea of a randomly drifting context signal that items get associated to.
\Textcite{Estes1955} presented the first model of this type and \textcite{Murdock1997} extended the TODAM2 model in this way to explain additional data.
An open question in these sort of models is, how the context at an earlier time can be re-instantiated to start the recall and how the context is advanced in the same way during recall as in the study phase to recall the remaining items.
As the signal drifts randomly a memory for the context signal itself would be needed.
The OSCAR model \parencite{Brown2000} solves part of this by using a deterministic context signal that is generated from multidimensional oscillators.
This allows to replay the exact same context signal once it has been reset.
It still does not answer how the context is re-instantiated to start the recall as this still requires knowledge of the oscillator states at begin of the study phase.

All of these context-based models cannot explain the asymmetric CRP curves.
The temporal context model (TCM, TODO ref), however, was specifically constructed to explain these free recall data.
It also uses a context signal, but this signal is updated by the studied (and recalled) items themself.
Each item recalls a prior context associated with that item to partially update the current context and the updated context gets associated with the studied item.
This solves the re-instantiation problem as the right cue can set the context to be similar to the study context to retrieve an item.
That retrieved item in turn updates the context to retrieve more items related to the updated context.
Thus, it is also available to appropriately advance the context after each recall.
The model will discussed in more detail in TODO\@.
But the TCM is not perfect.
It did not capture immediate free recall data involving short-term memory, even though it was presented as a single-store framework, i.e.\ a single memory for both STM and LTM\@.
TODO Davellar 2008 critique (does CUE capture the data?)

Many of these models are also vague on the exact processes of recall.
The ACT-R model of serial recall by \textcite{Anderson1997}, in which items are associated with their serial positions, describes detailed steps necessary for recall.




\begin{itemize}
    \item fuzzy temporal memory
    \item Start-End Model (SEM) (Henson, 1998) reproduces relative positional coding within groups (probably only one), not clear how endmarker can be generated (despite suggestions in the paper)
    \item Anderson and Matessa 1997 ACT-R model of serial recall, items are associated with serial position and grouped. Detailed steps of recall, but no neural implementation or specification of how exactly items are stored.
\end{itemize}


Mention?
\begin{itemize}
    \item context-signal-based models: grouping structure has to be known in advance for grouping effects
\end{itemize}


\subsection{Connectionist models}
Compared to the vast amount of mathematical models, there are much fewer connectionist models that try to ensure more biological realism.
Many of these models focus on reproducing low-level findings in the hippocampus such as sequence compression in replay \parencite{Levy2005} or place cells \parencite{Milford2004}.
The model presented by \textcite{Hasselmo2012}, might very well be the most comprehensive hippocampus model to date describing the storage of episodic memories as a spatial trajectory.
It addresses experimental data on place cells and the theta rhythm (TODO explained?).
A very recent model TODO ref Yu, constructs a three-layer spiking neural network that is able to encode a sequence over several iterations and replay it during a cycle of the theta rhythm.
These models, however, still leave a large gap to high-level cognitive behaviour as modelled by mathematical models.
In addition, the \textcite{Hasselmo2012} relies on hypothetical ``arc length'' cells to disambiguate memories \parencite[cp.][]{Robins2014}.

Nevertheless, there are some connectionist models that try to reduce this gap by addressing behavioural effects with a neural network implementation.
Namely, \textcite{Burgess1992} and \textcite{Burgess1996} proposed models for the articulary loop, \textcite{Norman2003} for recognition and familiarity effects, and \textcite{Botvinick2006} for immediate serial recall.
All of these models use rate based neurons as an abstraction.
While these are a useful tool to build tractable biological plausible models, one has to be careful to not introduce biologically implausible features.
For example, the noise introduced by discrete spikes is neglected.
In particular, \textcite{Norman2003} use a $k$-winner-take-all mechanism that is hard to realize in spiking neurons as it requires a fine balance of excitation and inhibition.
Potentially even more problematic is the use of back-propagation learning and a softmax function applied to the network outputs in the model by \textcite{Botvinick2006}.
While learning with back-propagation is a tremendously successful technique in machine learning, it is still unclear whether biological neural networks can implement this sort of learning.
But see work on feedback-alignment TODO and talk in lab TODO\@.
The softmax function is problematic for biological plausibility as it requires global knowledge of the network outputs.
TODO why is this a problem?

I only know of two memory-related models that address the these concerns about biological plausibility by using spiking neurons while at the same time connecting to behavioural data.
The first one is the ordinal serial encoding (OSE) model of serial recall by \textcite{Choo2010}.
As a primarily short-term memory model, it uses recurrently connected neurons to store the memory trace in neural activity.
This approach is also used to model a long-term memory component that can be attributed to hippocampal storage.
While this allows for a first approximation, a storage in synaptic weights seems more realistic for such a component.
The second model was specifically developed to model the storage of serial lists with hippocampus by \textcite{OliverTrujillo2014}.
It is able to reproduce neural data like replay and theta rhythm.
However, the length of stored lists is limited in similar fashion as STM capacity while long term memory is certainly able to learn longer lists.
Learning longer lists in the model would require the chaining of individual lists with different contexts, but not mechanism for this has been given in the model.
Another questionable feature is the usage of a clock signal that is adjusted to speed up compressed replay.


\subsection{Summary}

CUE\@: neural implementation (biological plausibility), includes recall/reinstantiation of context, (limited neural resources)


Further points (some maybe for conclusion, not introduction?):
\begin{itemize}
    \item Large scale modeling has new challenges.
    \item How do things interact?
    \item How are they coordinated?
    \item Increases to simulation speed (better neural representation, optimizer)
\end{itemize}
