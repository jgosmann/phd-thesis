\chapter{Introduction}

Memory in its different forms is an important aspect of human, but also animal cognition.
For example, it allows animals to return to previously visited water and food locations (TODO refs).
Prior experiences also allow to act more optimally in similar situations or avoid dangerous situations.
In this sense memory allows for adaptation on a faster time scale than genetic selection.
This is especially important in unstable and changing environments.
In humans, memory is also important to form social relationships, a shared culture, and even a functioning society.
Moreover, it contributes to our individual sense of self.
In general, the storage of information over time is a fundamental requirement in many cognitive systems and also from a computational point of view an addressable memory is important as it allows for more compact implementations than a pure state machine (TODO cite Gallistel).

Accordingly, there is a long history of memory research.
The well-known primacy and recency effect (discussed in more detail in TODO) have already been described by TODO Robinson, Brown 1926.
Nevertheless, there are still many open questions.
One important challenge that has to be solved by a memory system is the so-called \emph{stability-plasticity dilemma} (TODO ref Abaraham and A Robins 2005).
On the one hand, there is a need to quickly form new memories, sometimes even with a single exposure known as \emph{one-shot learning}.
On the other hand, such high plasticity can easily lead to overwriting of old memories rendering memory system useless.
TODO refo Buzsaki and others proposed multi-stage memory models where different memory system are in place for different timescales with different levels of plasticity.
However, this still leaves open TODO questions.

It also is a strong indication that different memory systems interact.
Though, it much of experimental research and modeling different memory systems have been treated as isolated.
This simplifies the analysis on a certain level, but to get a general understanding of memory as a whole the results need to integrated at some point.
Furthermore, many models of memory or learning focus on either small scale neural changes without any direct connection to behaviour or on the other extreme of describing behaviour with mathematical equations, but no solid grounding in biological plausibility.
So it is not only important to integrate our understanding of memory systems, but also to bridge the gap from neural mechanisms to behaviour.
This thesis will try to advance our understanding in this direction.

While this will still be comparably rough sketch and still fairly limited in memory systems covered, there are promising long-term prospects from a better understanding of human memory.
Many forms of memory loss are currently untreatable.
However, diseases associated with aging like Alzheimer's significantly impair the function of memory and are getting more common as our life-span increases due to other medical and nutritional advances.
(TODO stroke, hippocampal lesions?)
A better understanding of memory might allow us to devise better treatments or even stop and reverse the memory detoriation.
A potential route to this are memory implants that have already been demonstrated in rats by TODO Berger et all 2011.
For this sort of implant, an understanding of how memories are encoded is at least helpful if not crucial.

These are certainly strong motivators for research into memory, but it is also worthwhile to advance our general understanding of how the human brain works.
A question that has puzzled researchers for a long time and probably still will for a long time to come despite great advances.
An important step in testing our current understanding of the brain was the Spaun model (TODO ref).
A spiking neural network of XX neurons, thus grounded in biology, that can perform 8 different tasks.
Like a real brain it gets sensory input (low resolution black and white symbols) and produces a behavioural motor output with a simulated arm.
The number of tasks it can perform demonstrates that it is not a specialized model for a single task, but can switch between different tasks like a real brain.
Obviously, Spaun is still much simpler (and has much less neurons) like a real brain.
So there is no denying that it is still a long way to a full understanding of the whole brain.
But it incorporates a number of qualitative key aspects.
However, one key aspect is still missing.
While it can perform working memory dependent tasks and has simple reinforcement learning, it is missing a declarative and episodic long-term memory.
The work in this thesis is also a step towards providing Spaun with such a memory by integrating it with a short-term memory component similar to the one used in Spaun.


\section{Behavioural characterization of memory}
Memory is commonly characterized along two orthogonal dimensions:
by timescale and by type of information stored.
On the timescale axis one usually discriminates between short-term memory (STM), and long-term memory or short-term, long-term, and very long-term memory (vLTM).
However, there is no single agreed upon definition which timescales are encompassed in each of these terms.
Sometimes LTM is used to refer to vLTM, while sometimes LTM is in-between STM and vLTM\@.
As a rough orientation STM is usually taken to be on the sub-second timescale, up to a few seconds.
Very long-term memory is on timescales of weeks and up (TODO verify).
LTM, depending on the definition, covers timescales from minutes and up.
Moreover, the term working memory (WM) is often used to refer to active representations that allow direct mental manipulation.

In this thesis, I will use the term STM for timescales in the order of seconds and LTM for longer timescales, but separate from vLTM\@.
Moreover, I will assume STM to be based on neural-activity and thus this definition is somewhat in line with the definition of WM\@.

When classifying memory by type of information, a representation as a tree structure can be helpful.
On the highest level, we have the distinction between implicit and explicit memory.
Implicit memory does not allow for conscious access.
A typical example is procedural or motor memory like how to ride a bike.
Explicit memory allows for conscious access and is further subdivided into declarative and episodic memory.
The declarative memory allows us to store and reproduce facts like the birthday of a friend, whereas the episodic memory provides a recollection of life experiences.

Here, I will focus on declarative memory as this type of memory has been well studied in many memory experiments.


\section{Experimental findings in memory research}
Most memory experiments require the participants to memorize lists of words and recall them afterwards.
The words are typically chosen from the Toronto noun pool and are matched for semantic similarity and occurrence frequency (TODO verify, ref).
Usually an experiment will fall in one of two categories depending on how the subjects are required to recall the list items: in free recall experiments, the subjects are free to recall the items in any order; in serial recall experiments, the subjects are required to recall the items in order.

The hallmark finding in memory research, especially for serial recall experiments, are the primacy and recency effect.
Subjects tend to remember the start of list (primacy) and the end of list (recency) better (TODO fig).
Furthermore, if subjects in a serial recall experiment recall an item at the wrong position, a so-called \emph{transposition}, it is more likely to be an item that was close in the list than an item several positions spaced out.

In free recall experiments, participants usually start out by recalling the last items first.
This can be measured by the probability of first recall (TODO fig).
There is also the tendency, to recall items in forward direction and items close to the last recalled item.
The conditional response probability (CRP) measures this (TODO fig)
It gives the probability of how many items forward or backward (know as the lag) the next recall will jump.

In memory experiments the recall phase can directly follow the study phase, known as an immediate recall experiment, or a delay, potentially with a distractor task, can separate the two, known as delayed recall.
Sometimes a distraction task will be performed in between each list item which is known as continuous distractor recall.


\section{Neuroanatomy of memory}
TODO brain regions involved in short term recall

The hippocampus (HC) has been implicated in the acquisition of new episodic memories.
A finding derived from patients with hippocampal lesions, most famously HM who got his hippocampus removed due to severe epilepsy (TODO refs).
The hippocampus is named for his sea horse shaped structure.
It is located in the medial temporal lobe (TODO fig) and neighbours the subiculum and entorhinal cortex (EC).
The hippocampus is further divided into substructures CA3, CA1, and the dentate gyrus (DG) by its cytoarchitectural structure.
While the CA3 and CA1 regions consist mainly out of pyramidal neurons and about \SI{10}{\percent} GABAergic, inhibitory interneurons (TODO ref Freund and Buzsaki 1996), granule cells are the main constituent of the dentate gyrus.
Furthermore, the dentate gyrus has a large number of cells compared to CA3.
In humans the cell count of the DG is about twice as high as in CA3, whereas in rats DG has about five times more cells than the CA3 region.

The connectivity to, from, and within hippocampus is exceptionally well known.
There are two major pathways from entorhinal cortex.
The direct pathway originates in layer III of the entorhinal cortex and targets the distal apical dendrites of CA1 neurons.
The trisynaptic pathway originates in layer II and leads via the dentate gyrus, and CA3 also to CA1, but targeting the proximal dendrites (a signal thus crosses three synapses).
The connections from the dentate gyrus to the CA3 region in this is termed the mossy fiber pathway.
Each of these mossy fibers innervates about 15 neurons (TODO ref Claiborne, Amaral and Cowan 1986) which results in each CA3 pyramidal neuron receiving input from about 50 to 90 granule cells (TODO ref Squire, Shimamura, and Amaral 1989, p230).
The final bundle of connections from CA3 to CA1 is known as the Schaffer collateral pathway.

TODO further connections: EC to CA3 (mostly targeting same cells as from DG, Paxinos 2014), CA3 recurrent (single cell not innervated by more than 5\% of all CA3 cells Squire, Shimamura and Amaral 1989, p231)

Given these neuroanatomical properties, some of the hippocampal regions have been assumed to be involved in specific tasks.
The sparse cdoing and large cell count of the dentate gyrus led to the belief that it is responsible for pattern separation (TODO ref Rolls 2013).
The CA3 region with its recurrent connectivity could be involved in pattern completion or forward predictions (TODO ref Leutgeb and Leutgeb 2007, Rolls 2013).
Additional evidence for this comes from context dependent firing of these neurons (TODO how so? TODO ref Guzowski, KHierim, and Moser 2004, Leutgeb and Leutgeb 2007) and unimpaired recognition memory after lesioning the CA3 to CA1 connections despite impairing selective recall (TODO ref Brun et al 2002). 

theta rhythm, SPWs, grid and place cells

vLTM not hippocampal
Cerebellum for motor


\section{Memory models}
TODO discuss desired properties of models
TODO categorization by conceptual, abstract mathematical, and neural models

\subsection{Conceptual models}
\Textcite{Yntema1963} presented one of the first models of memory.
They conceptualized retrieval as a search process where each item in memory is associated with tags referencing further information.
For example, a time tag would encode the time an item was observed.
The whole description, however, is more based on how a memory system could be implemented on a classical von Neumann computer and does not consider if or how those operations could be neurally implemented.

To date, the most influential conceptual organization of working memory was proposed by \textcite{Baddeley1986}.
He proposed, based on experimental data, separate stores for visual and acoustic information, termed the visuospatial sketchpad and phonological loop respectively.
These are controlled by a central executive.
Furthermore, in \textcite{Baddeley2000} the model was extended with an episodic buffer for the binding of multimodal information and transfer to episodic long-term memory.
The main relevance of the model is that it informs us about the organization of (working) memory by modality; but it does less to elucidate mechanisms.

In general, conceptual models can give a high-level account and they can be evaluated with respect to their qualitative agreement to behavioural data.
However, they cannot provide us with quantitative predictions which makes a more rigorous validation difficult.
They also, do not explain the cognitive mechanisms in detail and an account of the neural implementation is completely out of their reach.


\subsection{Mathematical models}
Some of the weaknesses of conceptual models are addressed by mathematical models.
These models will make quantitative predictions about the behavioural data and describe underlying cognitive mechanisms to a varying degree, but do not provide a neural implementation.
There is a vast amount of such models and not all can be discussed here.
Thus I will focus on the most influential ones.

That list certainly contains the perturbation model for serial order by \textcite{Estes1972}, the free recall model Search of Associative Memory (SAM) by \textcite{Raaijmakers1981}, the recognition memory model Retrieving Effectively from Memory (REM) by \textcite{Shiffrin1997}, and the episodic memory model MINERVA2 by \textcite{Hintzman1988}.
The perturbation model an early attempt to provide a mathematical framework of how remembered item positions can drift over time.
In SAM cues are assembled in a short-term memory to retrieve associations from a long-term associative memory (the search part of the model).
In the REM model error prone copies of feature vectors derived from the study items are stored.
A recognition probe is matched to the stored feature vectors and a likelihood ratio of the match scores being generated by an old vs.\ a new item is calculated.
The MINERVA2 model also uses feature vectors that are stored as traces and can be probed by cues.

All of these models, and most other mathematical model, either assume item-to-item or position-to-item associations.
The primacy model by \textcite{Page1998} is worth noting because it uses a different approach.
In that model items are activated according to a primacy gradient, but the model is agnostic to how this gradient is generated.
This, however, also leaves that aspect underspecified.

Common to all of these models is that they are not demonstrating biological plausibility.
Thus it is unclear, whether any of the models can be implemented in a neural substrate while preserving the predictions.
Or even if, what the limitations with regard to noise and requirements for neural resources are.
Nevertheless, mathematical modelling is an important first step in figuring out what sort of processes are worth considering for a neural implementation.
Also, there are some reoccurring ideas in these models that are useful to consider in the context of a neural model.
For example, starting with \textcite{Anderson1973} many models have used random feature vectors to represent individual items.
That approach is similar to the Semantic Pointer Architecture (SPA) presented in Section TODO which can be implemented neurally with the methods of the Neural Engineering Framework (NEF, Section TODO).

An especially influential model based and random feature vectors was TODAM2 \parencite{Murdock1993}.
It was able to fit a large body of experimental data and in that aims to be a general theory for item recognition, serial order, and associative memory.
A neural implementation could very well be possible with the NEF, but has not been attempted so far.
However, \textcite{Choo2010} pointed out that the dimensionality of the vectors in the model increases with each stored item.
Thus, the requirement for neural resources grows unbounded.
It also worth noting, that TODAM2 does not give an account of how responses are generated.

Another useful idea, that had its origin in mathematical models, is the idea of a randomly drifting context signal that items get associated to.
\Textcite{Estes1955} presented the first model of this type and \textcite{Murdock1997} extended the TODAM2 model in this way to explain additional data.
An open question in these sort of models is, how the context at an earlier time can be re-instantiated to start the recall and how the context is advanced in the same way during recall as in the study phase to recall the remaining items.
As the signal drifts randomly a memory for the context signal itself would be needed.
The OSCAR model \parencite{Brown2000} solves part of this by using a deterministic context signal that is generated from multidimensional oscillators.
This allows to replay the exact same context signal once it has been reset.
It still does not answer how the context is re-instantiated to start the recall as this still requires knowledge of the oscillator states at begin of the study phase.

All of these context-based models cannot explain the asymmetric CRP curves.
The temporal context model (TCM, TODO ref), however, was specifically constructed to explain these free recall data.
It also uses a context signal, but this signal is updated by the studied (and recalled) items themself.
Each item recalls a prior context associated with that item to partially update the current context and the updated context gets associated with the studied item.
This solves the re-instantiation problem as the right cue can set the context to be similar to the study context to retrieve an item.
That retrieved item in turn updates the context to retrieve more items related to the updated context.
Thus, it is also available to appropriately advance the context after each recall.
The model will discussed in more detail in TODO\@.
But the TCM is not perfect.
It did not capture immediate free recall data involving short-term memory, even though it was presented as a single-store framework, i.e.\ a single memory for both STM and LTM\@.
TODO Davellar 2008 critique (does CUE capture the data?)

Many of these models are also vague on the exact processes of recall.
The ACT-R model of serial recall by \textcite{Anderson1997}, in which items are associated with their serial positions, describes detailed steps necessary for recall.




\begin{itemize}
    \item fuzzy temporal memory
    \item Start-End Model (SEM) (Henson, 1998) reproduces relative positional coding within groups (probably only one), not clear how endmarker can be generated (despite suggestions in the paper)
    \item Anderson and Matessa 1997 ACT-R model of serial recall, items are associated with serial position and grouped. Detailed steps of recall, but no neural implementation or specification of how exactly items are stored.
\end{itemize}


Mention?
\begin{itemize}
    \item context-signal-based models: grouping structure has to be known in advance for grouping effects
\end{itemize}


\subsection{Connectionist models}
Compared to the vast amount of mathematical models, there are much fewer connectionist models that try to ensure more biological realism.
Many of these models focus on reproducing low-level findings in the hippocampus such as sequence compression in replay \parencite{Levy2005} or place cells \parencite{Milford2004}.
The model presented by \textcite{Hasselmo2012}, might very well be the most comprehensive hippocampus model to date describing the storage of episodic memories as a spatial trajectory.
It addresses experimental data on place cells and the theta rhythm (TODO explained?).
This models, however, still leave a large gap to high-level cognitive behaviour as modelled by mathematical models.
In addition, the \textcite{Hasselmo2012} relies on hypothetical ``arc length'' cells to disambiguate memories \parencite[cp.][]{Robins2014}.

Nevertheless, there are some connectionist models that try to reduce this gap by addressing behavioural effects with a neural network implementation.
Namely, \textcite{Burgess1992} and \textcite{Burgess1996} proposed models for the articulary loop, \textcite{Norman2003} for recognition and familiarity effects, and \textcite{Botvinick2006} for immediate serial recall.
All of these models use rate based neurons as an abstraction.
While these are a useful tool to build tractable biological plausible models, one has to be careful to not introduce biologically implausible features.
For example, the noise introduced by discrete spikes is neglected.
In particular, \textcite{Norman2003} use a $k$-winner-take-all mechanism that is hard to realize in spiking neurons as it requires a fine balance of excitation and inhibition.
Potentially even more problematic is the use of back-propagation learning and a softmax function applied to the network outputs in the model by \textcite{Botvinick2006}.
While learning with back-propagation is a tremendously successful technique in machine learning, it is still unclear whether biological neural networks can implement this sort of learning.
But see work on feedback-alignment TODO and talk in lab TODO\@.
The softmax function is problematic for biological plausibility as it requires global knowledge of the network outputs.
TODO why is this a problem?

I only know of two memory-related models that address the these concerns about biological plausibility by using spiking neurons while at the same time connecting to behavioural data.
The first one is the ordinal serial encoding (OSE) model of serial recall by \textcite{Choo2010}.
As a primarily short-term memory model, it uses recurrently connected neurons to store the memory trace in neural activity.
This approach is also used to model a long-term memory component that can be attributed to hippocampal storage.
While this allows for a first approximation, a storage in synaptic weights seems more realistic for such a component.
The second model was specifically developed to model the storage of serial lists with hippocampus by \textcite{OliverTrujillo2014}.
It is able to reproduce neural data like replay and theta rhythm.
However, the length of stored lists is limited in similar fashion as STM capacity while long term memory is certainly able to learn longer lists.
Learning longer lists in the model would require the chaining of individual lists with different contexts, but not mechanism for this has been given in the model.
Another questionable feature is the usage of a clock signal that is adjusted to speed up compressed replay.


\subsection{Summary}

CUE\@: neural implementation (biological plausibility), includes recall/reinstantiation of context, (limited neural resources)


Further points (some maybe for conclusion, not introduction?):
\begin{itemize}
    \item Large scale modeling has new challenges.
    \item How do things interact?
    \item How are they coordinated?
    \item Increases to simulation speed (better neural representation, optimizer)
\end{itemize}
