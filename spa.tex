\chapter{The Semantic Pointer Architecture}
While the Neural Engineering Framework allows us to encode vectors into spiking neural network and transform them, but it does not tell us how to use those vectors to represent structured, conceptual, or symbolic information.
Different such methods could be devised, though in the context of the NEF the most widely used method is the Semantic Pointer Architecture (SPA\@; TODO ref).
The SPA is a specific instance of a Vector Symbolic Architecture (VSA\@; TODO ref).
In VSAs concepts are represented with vectors and linear and nonlinear operators are used to combine basic concepts in more complex structured representations.
Three types of operators can be considered essential in a VSA\@.

First, a measure of similarity
\begin{equation}
    \simmeasure: \mathbb{R}^{\dims} \times \mathbb{R}^{\dims} \longrightarrow \mathbb{R}
\end{equation}
for which we will use the normalized dot product
\begin{equation}
    \simmeasure(\vc x, \vc y) := \frac{\left\langle \vc x, \vc y \right\rangle}{\norm{\vc x} \cdot \norm{\vc y}}
\end{equation}
for the remainder of this thesis.
Second, a superposition operator
\begin{equation}
    \superpos: \mathbb{R}^{\dims} \times \mathbb{R}^{\dims} \longrightarrow \mathbb{R}^{\dims}
\end{equation}
that produces a vector similar to both inputs ($\vc x \sim \superpos(\vc x, \vc y) \sim \vc y$).
This is usually, and will be for the remainder of this thesis, simple addition, i.e. $\superpos(\vc x, \vc y) := \vc x + \vc y$.
Finally, a binding operator
\begin{equation}
    \bind: \mathbb{R}^{\dims_1} \times \mathbb{R}^{\dims_2} \longrightarrow \mathbb{R}^{\dims}
\end{equation}
with an approximate inverse or unbinding operation
\begin{equation}
    \bind^+: \mathbb{R}^d \times \mathbb{R}^{\dims_2} \longrightarrow \mathbb{R}^{\dims_1}
\end{equation}
that satisfies $\bind^+(\bind(\vc x, \vc y), \vc y) \approx \vc x$.
Note, that some proposed binding operations, like the tensor product (TODO ref), change the dimensionality of the vectors on binding.
This allows for a general exact inverse $\bind^{-1}(\bind(\vc x, \vc y), y) = \vc x$ to the binding operation.
However, this will increase the vector dimensionality with each binding and leads to scaling issues in a neural system (TODO ref).
Thus, we will only consider binding operations that preserve the vector dimensionality, i.e. $\dims_1 = \dims_2 = \dims$.
This implies, that in general such a binding cannot preserve all the information in both bound vectors.
It leads to a compressed representation.
To fully recover the stored information, clean-up memories will be necessary (TODO ref section).

At this point, it is useful to introduce three more definitions.
\begin{defn}[identity vector]
    A vector $\bid_{\bind}$ with the property $\bind(\vc x, \bid_{\bind}) = \vc x$ is called \emph{identity vector} under $\bind$.
\end{defn}
\begin{defn}[absorbing element]
    A vector $\bzero_{\bind}$ with the property $\bind(\vc x, \bzero_{\bind}) = c \cdot \bzero_{\bind}$ where $c \in \mathbb{R}$ is called \emph{absorbing element} under $\bind$.
\end{defn}
Such an absorbing element effectively destroys the information in the vector $\vc x$.
For that reason, absorbing elements should be avoided when constructing representations with binding.
Note that this definition slightly differs from the usual definition of absorbing elements by allowing for a scaling factor.
\begin{defn}[unitary vector]
    A vector $\vc u$ with the property $\langle \bind(\vc x, \vc u), \bind(\vc y, \vc u) \rangle = \langle \vc x, \vc y \rangle$ is called unitary.
\end{defn}
In other words, a unitary vector preserves the dot product under binding.
This is in analogy to unitary transformation matrices that also preserve the dot product.
It also implies that binding with a unitary vector preserves the length of the bound vector.


\section{Binding operations}
TODO some text

\subsection{Circular convolution}
The binding operator classically used in the SPA is circular convolution and was suggested by TODO ref Plate for his Holographic Reduced Representations (HRRs).
\begin{defn}[circular convolution binding]
    The circular convolution binding operator is given by
    \begin{equation}
        \bind_{\circledast}(\vc x, \vc y) := \vc x \circledast \vc y\ \text{with}\ \del{\vc x \circledast \vc y}_i = \sum_{j=0}^{d - 1} x_j y_{(i - j) \bmod d}
    \end{equation}
    and has the approximate inverse (TODO ref proof)
    \begin{equation}
        \bind^+_{\circledast}(\vc x, \vc y) = \vc x \circledast \vc y^+\ \text{with}\ \vc y^+ := \del{y_0, y_{d-1}, y_{d-2}, \dotsc, y_1}\Tr \text{.}
    \end{equation}
\end{defn}

The basic properties of
\begin{align}
    &\text{distributivity:} &(\vc x_1 + \vc x_2) \circledast \vc y &= \vc x_1 \circledast \vc y + \vc x_2 \circledast \vc y \text{,}\\
    &\text{associativity:} &(\vc x \circledast \vc y) \circledast \vc z &= \vc x \circledast (\vc y \circledast \vc z) \text{,} \\
    &\text{commutativity:} &\vc x \circledast \vc y &= \vc y \circledast \vc x
\end{align}
hold for circular convolution as a binding operator.
A useful property of circular convolution for the implementation in a neural network with the NEF is, that it becomes element-wise multiplication in the Fourier space defined by
\begin{equation}
    \vc x \circledast \vc y = \fouriermat^{-1}\sbr{(\fouriermat \vc x) \circ (\fouriermat \vc y)}
\end{equation}
where $\fouriermat$ is the discrete Fourier transform (DFT) matrix.
The linear transform with the DFT matrix can be put easily into the neural connection weights and the element-wise product can be done with well-optimized product network (TODO ref).

The expression in Fourier space also allows to derive the special elements of circular convolution.
The identity vector must not change the complex Fourier coefficient in the element-wise multiplication.
Thus, its Fourier coefficients must all be $1 + 0\iu$ and the identity vector is given by
\begin{equation}
    \bid_{\circledast} = (1, 0, 0, \dotsc, 0)\Tr \text{.}
\end{equation}
Furthermore, all vectors with Fourier coefficients $c_n \in \mathbb{C}$ that are of unit length ($\abs{c_n} = 1$) will be unitary as one can easily verify.
A trivial example of a unitary vector is the identity vector $\bid_{\circledast}$.
Finally, all vectors $(z, \dotsc, z)\Tr$ with $z \in \mathbb{R}$ are absorbing elements.

\subsection{Vector-derived transformation binding}
Circular convolution can be interpreted as moving one of the operands around in the $d$-dimensional space in a way defined by the other operand.
This leads to the question, whether there are other ways to project one vector to a new location based on the other vector.
One such way is what I call vector-derived transformation binding (VTB) which to my knowledge has not been described before.
\begin{defn}[vector-derived transformation binding, VTB]
    Given a $\dims' = \dims^{\frac{1}{2}} \in \mathbb{N}_{>0}$, the vector-derived transformation binding operator $\vtb: \mathbb{R}^{\dims} \times \mathbb{R}^{\dims} \longrightarrow \mathbb{R}^{\dims}$ is defined as
    \begin{equation}
        \vtb(\vc x, \vc y) := \bar{\mat V}_{\vc y} \vc x = \begin{bmatrix}
            \mat V_{\vc y} & 0 & 0 \\
            0 & \mat V_{\vc y} & 0 \\
            0 & 0 & \ddots
        \end{bmatrix} \vc x
    \end{equation}
    with
    \begin{equation}
        \mat V_{\vc y} = \dims^{\frac{1}{4}} \begin{bmatrix}
            y_1 & y_2 & \dotso & y_{\dims'} \\
            y_{\dims' + 1} & y_{\dims' + 2} & \dotso & y_{2\dims'} \\
            \vdots & \vdots & \ddots & \vdots \\
            y_{\dims - \dims' + 1} & y_{d - \dims' + 2} & \dotso & y_{\dims}
        \end{bmatrix} \text{.}
    \end{equation}
    The approximate inverse is given by
    \begin{equation}
        \vtb^+(\vc x, \vc y) = \bar{\mat V}_{\vc y}\Tr = \begin{bmatrix}
            V_{\vc y}\Tr & 0 & 0 \\
            0 & V_{\vc y}\Tr & 0 \\
            0 & 0 & \ddots
        \end{bmatrix} \vc x \text{.}
    \end{equation}
\end{defn}
This binding method is based on the fact that in the SPA vectors are usually randomly generated and uniformly distributed with identically distributed components.
In that case each subvector (e.g., each row in $\mat V_{\vc y}$) is also uniformly distributed with identically distributed components.
Furthermore, for high-dimensional vector spaces almost all (uniformly sampled) vectors are orthogonal and semantic pointers have usually unit-length.
Thus, the matrix $\bar{\mat V}_{\vc y}$ is almost orthogonal with the implication $\bar{\mat V}_{\vc y}\Tr \bar{\mat V}_{\vc y} \approx \imat$.
Vectors $\vc y$ that give a perfectly orthogonal matrix $\mat V_{\vc y}$, will be unitary.
One special unitary vector is the identity vector.
\begin{corollary}[VTB identity vector]
    The identity vector for VTB is given by
    \begin{equation}
        \sbr{\bid_{\ped{V}}}_i = \left\{ \begin{array}{ll}
                \dims^{\frac{1}{4}} & i \in \cbr{(k - 1) \dims' + k : k \leq \dims', k \in \mathbb{N}_{>0}} \\
                0 & \text{otherwise}
        \end{array}\right. \text{.}
    \end{equation}
    \begin{proof}
        $\mat V_{\bid_{\ped{V}}} = I\ \Rightarrow\ \bar{\mat V}_{\bid_{\ped{V}}} = \imat$
    \end{proof}
\end{corollary}

\begin{corollary}[VTB distributivity]
    VTB is distributive: $\vtb(\vc x_1 + \vc x_2, \vc y) = \vtb(\vc x_1, \vc y) + \vtb(\vc x_2, \vc y)$ and $\vtb(\vc x, \vc y_1 + \vc y_2) = \vtb(\vc x, \vc y_1) + \vtb(\vc x, \vc y_2)$.
    \begin{proof}
        By applying the definitions for both directions of the distributivity:
        \begin{itemize}
            \item $\vtb(\vc x_1 + \vc x_2, \vc y) = \bar{\mat V}_{\vc y} \del{\vc x_1 + \vc x_2} = \bar{\mat V}_{\vc y} \vc x_1 + \bar{\mat V}_{\vc y} \vc x_2 = \vtb(\vc x_1, \vc y) + \vtb(\vc x_2, \vc y)$
            \item $\vtb(\vc x, \vc y_1 + \vc y_1) = \bar{\mat V}_{\vc y_1 + \vc y_2} \vc x = \del{\bar{\mat V}_{\vc y_1} + \bar{\mat V}_{\vc y_2}} \vc x = \bar{\mat V}_{\vc y_1} \vc x + \bar{\mat V}_{\vc y_2} \vc x = \vtb(\vc x, \vc y_1) + \vtb(\vc x, \vc y_2)$
    \end{itemize}
    \end{proof}
\end{corollary}
In contrast to circular convolution, VTB is neither commutative
\begin{equation}
    \vtb(\vc x, \vc y) = \bar{\mat V}_{\vc y} \vc x \neq \bar{\mat V}_{\vc x} \vc y = \vtb(\vc y, \vc x) \text{,}
\end{equation}
nor associative
\begin{equation}
    \vtb(\vc x, \vtb(\vc y, \vc z)) = \bar{\mat V}_{\bar{\mat V}_{\vc z} \vc y} \vc x \neq \bar{\mat V}_{\vc z} \bar{\mat V}_{\vc y} \vc x = \vtb(\vtb(\vc x, \vc y), \vc z) \text{.}
\end{equation}
This implies that unlike circular convolution multiple binding cannot be undone in a single step, but a separate unbinding step is required for each binding.

\subsection{Comparison of circular convolution and vector-derived transformation binding}


\section{Structured representations}

Representations, the Semantic Pointers, in the SPA act similar to pointers in computer science.
They can be dereferenced to access information not directly contained in that representation.
But opposed to computer science pointers, they are also semantic by capturing semantic relations with their distance in vector space.



mention Spaun and other models using SPA
