\chapter{The Semantic Pointer Architecture}
While the Neural Engineering Framework allows us to encode vectors into spiking neural network and transform them, but it does not tell us how to use those vectors to represent structured, conceptual, or symbolic information.
Different such methods could be devised, though in the context of the NEF the most widely used method is the Semantic Pointer Architecture (SPA\@; TODO ref).
The SPA is a specific instance of a Vector Symbolic Architecture (VSA\@; TODO ref).
In VSAs concepts are represented with vectors and linear and nonlinear operators are used to combine basic concepts in more complex structured representations.
Three types of operators can be considered essential in a VSA\@.

First, a measure of similarity
\begin{equation}
    \simmeasure: \mathbb{R}^{\dims} \times \mathbb{R}^{\dims} \longrightarrow \mathbb{R}
\end{equation}
for which we will use the normalized dot product
\begin{equation}
    \simmeasure(\vc x, \vc y) := \frac{\left\langle \vc x, \vc y \right\rangle}{\norm{\vc x} \cdot \norm{\vc y}}
\end{equation}
for the remainder of this thesis.
Second, a superposition operator
\begin{equation}
    \superpos: \mathbb{R}^{\dims} \times \mathbb{R}^{\dims} \longrightarrow \mathbb{R}^{\dims}
\end{equation}
that produces a vector similar to both inputs ($\vc x \sim \superpos(\vc x, \vc y) \sim \vc y$).
This is usually, and will be for the remainder of this thesis, simple addition, i.e. $\superpos(\vc x, \vc y) := \vc x + \vc y$.
Finally, a binding operator
\begin{equation}
    \bind: \mathbb{R}^{\dims_1} \times \mathbb{R}^{\dims_2} \longrightarrow \mathbb{R}^{\dims}
\end{equation}
with an approximate inverse or unbinding operation
\begin{equation}
    \bind^+: \mathbb{R}^d \times \mathbb{R}^{\dims_2} \longrightarrow \mathbb{R}^{\dims_1}
\end{equation}
that satisfies $\bind^+(\bind(\vc x, \vc y), \vc y) \approx \vc x$ and $\vc x \not \sim \bind(\vc x, \vc y) \not \sim \vc y$ (except identity FIXME better explanation/requirement).
Note, that some proposed binding operations, like the tensor product (TODO ref), change the dimensionality of the vectors on binding.
This allows for a general exact inverse $\bind^{-1}(\bind(\vc x, \vc y), y) = \vc x$ to the binding operation.
However, this will increase the vector dimensionality with each binding and leads to scaling issues in a neural system (TODO ref).
Thus, we will only consider binding operations that preserve the vector dimensionality, i.e. $\dims_1 = \dims_2 = \dims$.
This implies, that in general such a binding cannot preserve all the information in both bound vectors.
It leads to a compressed representation.
To fully recover the stored information, clean-up memories will be necessary (TODO ref section).

At this point, it is useful to introduce three more definitions.
\begin{defn}[identity vector]
    A vector $\bid_{\bind}$ with the property $\bind(\vc x, \bid_{\bind}) = \vc x$ is called \emph{identity vector} under $\bind$.
\end{defn}
\begin{defn}[absorbing element]
    A vector $\bzero_{\bind}$ with the property $\bind(\vc x, \bzero_{\bind}) = c \cdot \bzero_{\bind}$ where $c \in \mathbb{R}$ is called \emph{absorbing element} under $\bind$.
\end{defn}
Such an absorbing element effectively destroys the information in the vector $\vc x$.
For that reason, absorbing elements should be avoided when constructing representations with binding.
Note that this definition slightly differs from the usual definition of absorbing elements by allowing for a scaling factor.
\begin{defn}[unitary vector]
    A vector $\vc u$ with the property $\langle \bind(\vc x, \vc u), \bind(\vc y, \vc u) \rangle = \langle \vc x, \vc y \rangle$ is called unitary.
\end{defn}
In other words, a unitary vector preserves the dot product under binding.
This is in analogy to unitary transformation matrices that also preserve the dot product.
It also implies that binding with a unitary vector preserves the length of the bound vector.


\section{Binding operations}
TODO some text

\subsection{Circular convolution}
The binding operator classically used in the SPA is circular convolution and was suggested by TODO ref Plate for his Holographic Reduced Representations (HRRs).
\begin{defn}[circular convolution binding]
    The circular convolution binding operator is given by
    \begin{equation}
        \bind_{\circledast}(\vc x, \vc y) := \vc x \circledast \vc y\ \text{with}\ \del{\vc x \circledast \vc y}_i = \sum_{j=0}^{d - 1} x_j y_{(i - j) \bmod d}
    \end{equation}
    and has the approximate inverse (TODO ref proof)
    \begin{equation}
        \bind^+_{\circledast}(\vc x, \vc y) = \vc x \circledast \vc y^+\ \text{with}\ \vc y^+ := \del{y_0, y_{d-1}, y_{d-2}, \dotsc, y_1}\Tr \text{.}
    \end{equation}
\end{defn}

The basic properties of
\begin{align}
    &\text{distributivity:} &(\vc x_1 + \vc x_2) \circledast \vc y &= \vc x_1 \circledast \vc y + \vc x_2 \circledast \vc y \text{,}\\
    &\text{associativity:} &(\vc x \circledast \vc y) \circledast \vc z &= \vc x \circledast (\vc y \circledast \vc z) \text{,} \\
    &\text{commutativity:} &\vc x \circledast \vc y &= \vc y \circledast \vc x
\end{align}
hold for circular convolution as a binding operator.
A useful property of circular convolution for the implementation in a neural network with the NEF is, that it becomes element-wise multiplication in the Fourier space defined by
\begin{equation}
    \vc x \circledast \vc y = \fouriermat^{-1}\sbr{(\fouriermat \vc x) \circ (\fouriermat \vc y)}
\end{equation}
where $\fouriermat$ is the discrete Fourier transform (DFT) matrix.
The linear transform with the DFT matrix can be put easily into the neural connection weights and the element-wise product can be done with well-optimized product network (TODO ref).

The expression in Fourier space also allows to derive the special elements of circular convolution.
The identity vector must not change the complex Fourier coefficient in the element-wise multiplication.
Thus, its Fourier coefficients must all be $1 + 0\iu$ and the identity vector is given by
\begin{equation}
    \bid_{\circledast} = (1, 0, 0, \dotsc, 0)\Tr \text{.}
\end{equation}
Furthermore, all vectors with Fourier coefficients $c_n \in \mathbb{C}$ that are of unit length ($\abs{c_n} = 1$) will be unitary as one can easily verify.
A trivial example of a unitary vector is the identity vector $\bid_{\circledast}$.
Finally, all vectors $(z, \dotsc, z)\Tr$ with $z \in \mathbb{R}$ are absorbing elements.

\subsection{Vector-derived transformation binding}
Circular convolution can be interpreted as moving one of the operands around in the $d$-dimensional space in a way defined by the other operand.
This leads to the question, whether there are other ways to project one vector to a new location based on the other vector.
One such way is what I call vector-derived transformation binding (VTB) which to my knowledge has not been described before.
\begin{defn}[vector-derived transformation binding, VTB]
    Given a $\dims' = \dims^{\frac{1}{2}} \in \mathbb{N}_{>0}$, the vector-derived transformation binding operator $\vtb: \mathbb{R}^{\dims} \times \mathbb{R}^{\dims} \longrightarrow \mathbb{R}^{\dims}$ is defined as
    \begin{equation}
        \vtb(\vc x, \vc y) := \bar{\mat V}_{\vc y} \vc x = \begin{bmatrix}
            \mat V_{\vc y} & 0 & 0 \\
            0 & \mat V_{\vc y} & 0 \\
            0 & 0 & \ddots
        \end{bmatrix} \vc x
    \end{equation}
    with
    \begin{equation}
        \mat V_{\vc y} = \dims^{\frac{1}{4}} \begin{bmatrix}
            y_1 & y_2 & \dotso & y_{\dims'} \\
            y_{\dims' + 1} & y_{\dims' + 2} & \dotso & y_{2\dims'} \\
            \vdots & \vdots & \ddots & \vdots \\
            y_{\dims - \dims' + 1} & y_{d - \dims' + 2} & \dotso & y_{\dims}
        \end{bmatrix} \text{.}
    \end{equation}
    The approximate inverse is given by
    \begin{equation}
        \vtb^+(\vc x, \vc y) = \bar{\mat V}_{\vc y}\Tr = \begin{bmatrix}
            V_{\vc y}\Tr & 0 & 0 \\
            0 & V_{\vc y}\Tr & 0 \\
            0 & 0 & \ddots
        \end{bmatrix} \vc x \text{.}
    \end{equation}
\end{defn}
This binding method is based on the fact that in the SPA vectors are usually randomly generated and uniformly distributed with identically distributed components.
In that case each subvector (e.g., each row in $\mat V_{\vc y}$) is also uniformly distributed with identically distributed components.
Furthermore, for high-dimensional vector spaces almost all (uniformly sampled) vectors are orthogonal and semantic pointers have usually unit-length.
Thus, the matrix $\bar{\mat V}_{\vc y}$ is almost orthogonal with the implication $\bar{\mat V}_{\vc y}\Tr \bar{\mat V}_{\vc y} \approx \imat$.
Vectors $\vc y$ that give a perfectly orthogonal matrix $\mat V_{\vc y}$, will be unitary.
One special unitary vector is the identity vector.
\begin{corollary}[VTB identity vector]
    The identity vector for VTB is given by
    \begin{equation}
        \sbr{\bid_{\ped{V}}}_i = \left\{ \begin{array}{ll}
                \dims^{\frac{1}{4}} & i \in \cbr{(k - 1) \dims' + k : k \leq \dims', k \in \mathbb{N}_{>0}} \\
                0 & \text{otherwise}
        \end{array}\right. \text{.}
    \end{equation}
    \begin{proof}
        $\mat V_{\bid_{\ped{V}}} = I\ \Rightarrow\ \bar{\mat V}_{\bid_{\ped{V}}} = \imat$
    \end{proof}
\end{corollary}

\begin{corollary}[VTB distributivity]
    VTB is distributive: $\vtb(\vc x_1 + \vc x_2, \vc y) = \vtb(\vc x_1, \vc y) + \vtb(\vc x_2, \vc y)$ and $\vtb(\vc x, \vc y_1 + \vc y_2) = \vtb(\vc x, \vc y_1) + \vtb(\vc x, \vc y_2)$.
    \begin{proof}
        By applying the definitions for both directions of the distributivity:
        \begin{itemize}
            \item $\vtb(\vc x_1 + \vc x_2, \vc y) = \bar{\mat V}_{\vc y} \del{\vc x_1 + \vc x_2} = \bar{\mat V}_{\vc y} \vc x_1 + \bar{\mat V}_{\vc y} \vc x_2 = \vtb(\vc x_1, \vc y) + \vtb(\vc x_2, \vc y)$
            \item $\vtb(\vc x, \vc y_1 + \vc y_1) = \bar{\mat V}_{\vc y_1 + \vc y_2} \vc x = \del{\bar{\mat V}_{\vc y_1} + \bar{\mat V}_{\vc y_2}} \vc x = \bar{\mat V}_{\vc y_1} \vc x + \bar{\mat V}_{\vc y_2} \vc x = \vtb(\vc x, \vc y_1) + \vtb(\vc x, \vc y_2)$
    \end{itemize}
    \end{proof}
\end{corollary}
In contrast to circular convolution, VTB is neither commutative
\begin{equation}
    \vtb(\vc x, \vc y) = \bar{\mat V}_{\vc y} \vc x \neq \bar{\mat V}_{\vc x} \vc y = \vtb(\vc y, \vc x) \text{,}
\end{equation}
nor associative
\begin{equation}
    \vtb(\vc x, \vtb(\vc y, \vc z)) = \bar{\mat V}_{\bar{\mat V}_{\vc z} \vc y} \vc x \neq \bar{\mat V}_{\vc z} \bar{\mat V}_{\vc y} \vc x = \vtb(\vtb(\vc x, \vc y), \vc z) \text{.}
\end{equation}
This implies that unlike circular convolution multiple binding cannot be undone in a single step, but a separate unbinding step is required for each binding.

\subsection{Comparison of circular convolution and vector-derived transformation binding}
Both circular convolution and VTB are compressed binding operations.
Because of the lossy compression, we lose some information in each binding which makes it increasingly harder to recover the original unbound vectors.
To combat this effect (and the effect of neuron noise) clean-up memories like the one by TODO ref Stewart paper are required.

In addition to the information loss, the binding operations will change the length of the vector (if neither operand is unitary).
This can be a problem in a neural representation as neurons will saturate and might not represent the vector accurately anymore.
In particular, neural ensembles in the NEF are optimized for a certain representational space, usually a hyper-ball with a given radius $\radius$.
It is convenient to set $\radius = 1$ and try to keep the Semantic Pointer vectors at unit-length.

Figure TODO shows how the mean length of random vectors repeatedly bound with itself changes.
For the circular convolution the length increases exponentially, while VTB show a much slower increase in the length.
This is beneficial for the usage in a neural network.
Moreover, VTB preserves more information of the bound vectors as shown in Figure TODO.
After repeated binding with itself and then the same number of unbindings, the resulting VTB vector is more similar to the original vector than the circular convolution vector.

While binding vectors with themselves can sometimes be useful (e.g., for generating Semantic Pointers with a successive relationship like position indices), it is much more common to bind randomly sampled vectors.
Figure TODO shows the same experiment where a random vector was used in each binding.
In this case, the vector length will decrease to zero, even if it might increase in some of the early bindings.
Again, this decrease is much quicker for circular convolution binding than for VTB and the latter method also preserves more of the similarity across bindings.

It is conceivable that the problems with scaling of the vector length can be fixed by normalizing after each binding.
This, however, will not affect the loss of information in each binding (Figure TODO).
Also, implementing normalization in a neural network is notoriously difficult because of the involved division with an unbounded output as the divisor approaches zero.
Good approximations of normalization are only possible for a defined and finite input range.
It is worth to note that the neurons in the NEF will perform some sort of soft normalization for large values as the neuron's firing rates will saturate.
But this only affects vectors exceeding a certain length and can lead to other distortion in the representation.

Another approach to prevent the growth or decay of the vector length and even prevent the information loss, is the usage of unitary vectors.
These will keep the vector length constant and perform lossless binding due to their perfect inverse.
Note that binding two unitary vectors will give another unitary vector.
Thus, repeated binding is not a problem.
However, the scaling properties of VSAs are based on the fact that the number of almost orthogonal vectors that fits into a vector space grows exponentially with the dimensionality of that space.
Because not all vectors are unitary, this scaling property might be lost when restricted to unitary vectors.
In fact, the number of almost orthogonal unitary circular convolution vectors grows only linear (TODO proof).
For VTB the number of unitary vectors still grows exponentially (TODO proof), but these are picked out of a space that effectively is only $\sqrt{\dims}$-dimensional.
It might be best to use unitary vectors only for those Semantic Pointers that will be repeatedly used in bindings.
But it is also worth keeping in mind that achieving the theoretical limit of almost orthogonal vectors in a space is hard and unsolved (TODO accurate?) mathematical problem related to sphere packing (TODO ref).
Thus, the practical scaling of the number of useable vectors might be closer to linear for both unitary and non-unitary vectors.

So far VTB looks like the better choice for a binding operation.
But it is not without downsides.
In contrast to circular convolution, it is not associative and commutative.
While the desirability of commutativity depends on the employed representation scheme, the non-associativity implies that each binding has to be undone in an individual step, while circular convolution allows to undo a chain of bindings in a single step if the vector representing that chain is available.
Thus, circular convolution can allow to recover information more quickly.
Ultimately, this will be a question what binding operation the brain applies (if the SPA is at all related ot what the brain does).
Potentially, the binding operations lead to different timing predictions as unbinding with the VTB will take more time.
Deriving such predictions and testing them experimentally is, however, out of the scope of this thesis.

Finally, we have to consider the neural implementation of these binding operations.
Both essentially require a set of multiplication networks.
For the circular convolution, the DFT (and inverse DFT) can be implemented in feed-forward connection weights that do not affect the number of neurons required.
For each of the input vectors $\dims$ Fourier complex coefficients will be produced, but as the inputs are real-valued, half of these will be the complex conjugate of the other half.
Thus, only $\dims / 2$ coefficients have to be considered.
Each coefficient is a complex number multiplied with one coefficient of the other vector.
That results in for real-valued multiplies per coefficient.
In total, $2\dims$ multiplications will be required for a circular convolution.
For VTB, there are $d^{1/2}$ multiplications of $d^{1/2} \times d^{1/2}$ matrices with a vector, resulting in a total of $d^{3/2}$ multiplications.
Thus, the VTB requires more neural resources as a larger number of multiplication networks is required.
It should be noted, that for either binding method the binding with a fixed vector can be implemented purely in the connection weights as it reduces to a simple matrix multiplication in either case.

Despite VTB having many advantages over circular convolution, I decided to use circular convolution in the memory model.
The main reason is that support for circular convolution is already implemented in Nengo and the model does not use a lot of binding operations.
Nevertheless, it would be interesting to switch the model over to VTB in the future.


\section{Structured representations}

Representations, the Semantic Pointers, in the SPA act similar to pointers in computer science.
They can be dereferenced to access information not directly contained in that representation.
But opposed to computer science pointers, they are also semantic by capturing semantic relations with their distance in vector space.



mention Spaun and other models using SPA
