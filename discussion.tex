\chapter{Discussion}
In this thesis, I presented the context-unified encoding (CUE) model.
To my knowledge, it is the first spiking neural network model of human memory that integrates activity-based short-term memory and weight-based long-term memory.
The same model matches a variety of behavioural data from serial and free recall experiments, but in contrast to previous models provides a neural mechanistic explanation.

The CUE model exhibits many of the hallmark findings in memory research.
It shows the primacy and recency effect in immediate serial and free recall.
These effects get attenuated in delayed free recall, but in continuous distractor free recall the recency effect reappears.
Furthermore, the model was found to make very view transposition errors in serial recall and if it does so nearby items will be transposed.
In the free recall conditions, the model tends to start with items at the end of the list, recall nearby items together, and favour recall in forward direction.
Introducing delays and distractors attenuates these effects.
All of this matches the findings from experiments with human subjects.

Not only the qualitative effects are reproduced, but also the quantitative match to the data is good.
Only very few significant differences close to the number of differences expected by chance were found.
One of these differences is worth to be considered in more detail: the model predicts a too strong forward bias in delayed free recall with both the lag \num{1} and \num{2} values of the CRP curve being significantly above the experimentally found values.
Interestingly, this is also highlighted as the least well matched aspect in the original TCM \parencite{Howard2002}.
While in that publication the TCM prediction is closer to the experimental data, the TCM prediction from a more recent paper \parencite{Sederberg2008} is closer to the CUE model prediction.
This makes it likely that the difference is not based on pure statistical chance, but that both the TCM and CUE model do not capture an essential aspect of memory, potentially related to the evolution of the context signal, that leads to reduced forward bias in delayed free recall.
It remains for future work, to precisely identify the reason for this mismatch and extend the model.

Opposed to pure math models, the implementation as a spiking neural network allows to compare and validate the model against data from neural recordings in addition to the behavioural in the future.
In \cref{sec:aml-neural}, it was already demonstrated that the isolated mechanism of association learning is able to explain neural data.
Unfortunately, neural data recorded from human in memory experiments is still scarce because invasive recordings can only be done when such recordings are required for medical reasons.
Nevertheless, implementing models with spiking neurons is worthwhile for several other reasons, despite the more complicated model construction and increased simulations times.
Drug effects, like scopolamine, can be more readily modelled as was done with CUE model.
Also a higher degree of biological plausibility is achieved as one is forced to consider, for example, spiking noise and synaptic time constants.
This prevents common assumptions like arbitrary precision or perfectly orthogonal vectors made in many math models.

The spiking neural implementation also helps constraining many parameter values.
Synaptic time constants, membrane time constants, and similar cellular physical quantities can be set to biological plausible values constrained by experimental findings.
These are fixed parameters that have not been adjusted for matching the data.
Similarly, as in the NEF most connection weights are directly determined by least-squares minimization to implement a given function determined by the prescribed model architecture, the connection weights are fixed as well.
This leaves the model with very few free parameters.

To match the immediate serial recall, only two parameters were adjusted: the bias of the null choice $\minev$ and the input noise standard deviation $\recnoise$ in recall.
Both account for the fact that the recall network was restricted to recalling the items used within the memory experiment, while in reality a number of other items might interfere with the recall process.
For free recall experiments, one additional parameter $\psi$ is added that determines the probability to use the serial recall strategy even for free recall.
(For serial recall, a fixed values of $\psi = 1$ is implied as no free recall is allowed.)
Furthermore, in experiments with delay periods, a distractor rate $\drate$ needs to be set.
Lastly, to simulate the effect of scopolamine the AML learning rate $\eta$ was adjusted.
However, in non-scopolamine conditions, it was treated as a fixed parameter and set to a value high enough to learn associations until the threshold for inhibition was achieved within the presentation duration.
Even higher values would not have any effect as long as it does not largely exceed the inverse of the synaptic time delay of the inhibition.

While few free parameters are desirable with respect to model parsimony, they should also be assigned similar values to model related experimental conditions.
This is mostly the case for the CUE model.
The bias of the null choice in recall ranges from \numrange{0.03}{0.04} and values get monotonously smaller as the task difficulty increases with additional delays.
This corresponds to plausible longer recall attempts in more difficult experimental conditions.
Only a small difference is also observed in the distractor rates (\numrange{0.03}{0.04}) and the probability of using a serial recall strategy (zero for delayed recall and \num{0.1} in all other free recall conditions).
However, the noise standard deviation $\recnoise$ in recall differs by a factor of more than \num{1.5} without a clear relation to the experimental condition.
It is hard to hypothesize potential reasons for this difference as the parameter is accounting for things not explicitly modeled in recall.

It is also of interest how robust the model is against parameter changes.
I have not done a formal analysis of this because the model simulation times are prohibitive.
However, this also means that only a small set of parameter values without a lot of fine tuning has been tested (less than \num{200} combinations summed over all experimental conditions).
Given that finding the right parameters with few simulations is less likely if the model were highly sensitive to the parameter choice, a sufficient robustness to the exact choice of parameter values can be expected.

The CUE model is based on prior model of memory, but improves on them in important ways.
With regard to the OSE model two main advancements can be stated.
First, The episodic memory buffer has been replaced with a much more plausible long-term memory mechanism that relies on synaptic-weight changes rather than reverberating neural activity.
Second, the CUE model also implements the mechanism providing the position tags fully in spiking neurons.

Implementing a long-term memory component based on the TCM in a spiking neural network provides a strong support for the biological plausibility of the TCM that previously was missing.
Certain simplifications of the TCM equations in this process to facilitate this implementation highlight which aspects of the TCM are essential and which do not contribute to the explanation of the data.
In particular, it also shows that certain assumptions, like perfectly orthogonal vectors, useful in the mathematical analysis, are not essential.
In addition, the modified TCM has been extended with a short-term memory component in the CUE model.
While the TCM has been posited as a single-store model, this has been criticized \parencite{Davelaar2008}.
The CUE model demonstrates that treating the TCM as part of a multi-store model is not unreasonable, provides good matches to the free recall data and in addition allows to match serial recall data.
Finally, the recall process in the TCM was not modeled in a particular biological plausible way and has been replaced with a more plausible spiking neural mechanism (\cref{sec:recall}).

In the broader context of memory models, the CUE model is unique as providing a low-level spiking network implementation, but matching high-level behavioural data.
This includes the recall process that is not explicitly modeled in many other models.
Furthermore, due to the item based context, there is no reinstantiation problem found in most context-based memory models (except the TCM and CUE model).

A key part of the CUE model is the association matrix learning rule.
It provides insight into how one-shot learning without catastrophic forgetting is possible.
Several options and their biological plausibility of how this learning rule might be realized in the brain have been discussed in \cref{sec:aml}.
The main point there was that either some form of weight-sharing or symmetric decoder matrix is needed, or the input needs to be transformed into a sparse representation.
The dentate gyrus of the hippocampus exhibits such sparse firing and is implicated in associative learning, giving support to the latter hypothesis.
But I provided also evidence that a symmetric decoder matrix might be learned in a biological plausible way.
Moreover, using the AML for learning simple pairwise associations, allows to reproduce changes in firing rates that have been observed in recordings from human hippocampus.
These results are not only of interest for the CUE and TCM models, but many other cognitive models that assume the storage of associations in a similar association matrix without further explanation of how these associations are learned.


\section{Anatomical mapping}

Given that the CUE model is neural, it is of interest to consider how parts of the model map to brain areas.
\Textcite{howard2005} proposed a mapping of the TCM model to brain areas that applies to a large degree also to the TCM-based part of the CUE model.
There are however some details to be reconsidered and the OSE-based short-term part has obviously not been discussed.

The medial temporal lobe (MTL) is known to be essential for free recall.
Damage to the MTL is detrimental to free recall performance \parencite{graf1984}.
Thus, we can assume the TCM related parts of the model to reside in the MTL (TODO figure).

More precisely, the context storage network can be mapped  onto parahippocampal areas, in particular the entorhinal cortex (EC).
Its properties are consistent with the storage of non-spatial memories for tens of seconds.
In delay periods stimulus dependent persistent activity can be observed \parencite{suzuki1997,young1997}.
\Textcite{quirk1992} showed EC has a higher mean firing rate than hippocampus which is not caused by short bursts and is thus compatible with the sustained maintenance of neural firing.
Also, the electrophysiological properties of the EC support integration \parencite{egorov2002}.
These findings are, however, based on the intrinsic cell properties, whereas the CUE model uses recurrent connectivity for integration instead.
Note, that the context network does contain integration ensembles that maintain the context signal over the timespan of seconds.

While the EC firing is modulated to some degree by the position, this coding is more noisy than in the hippocampus \parencite{quirk1992}.
This indicates that EC codes for additional information.
\Textcite{Frank2000} have shown that superficial EC employs retrospective coding, that it differentiates visits of the same position by the history leading up to that visit.
This is consistent with a context signal encoding the history of items leading up to the current item.

The other major component to map to brain structures are the learning and retrieval of associations in the $\mft$ and $\mtf$ matrices.
\Textcite{howard2005} stated that the $\mft$ matrix might not be implemented by a single anatomical region due to its complicated structure.
However, the updating equation for $\mft$ in the CUE model has been simplified which makes the correspondence to a single region more plausible.
The learning of new associations in these matrices is attributed to the hippocampus by \textcite{howard2005}.
TODO more details.
Though, they do not consider the retrieval to be dependent on the hippocampus because TODO\@.

TODO because of striatal association learning?
Simple associations of same modality might be learned there, but cross-modality and integration (i.e.\ of item and context) might happen in hippocampus.
But striatum seems to be mostly for reward associations like in Pavlovian conditioning and other reward/punishment learning.

The CUE model provides a more detailed description of the updating of the association matrices due to the neural implementation by means of the AML\@.
In the learning network we get recurrent connectivity between TODO\@.
This is analogous to recurrent connectivity in the CA3 region of hippocampus.
TODO additional input via different pathways.
Moreover, the AML highlighted the need to incorporate the decoder matrix $\mdec\Tr$ into the connections.
While I have shown it might be plausible that this connectivity is learned, the matrix could be reduced to the identity if the input ensemble were to provide orthogonalized inputs.
The dentate gyrus, providing input to CA3 is commonly assumed to perform such orthogonalization given its large neuron count, sparse firing, and neurogenesis (TODO refs).
Thus, while the CUE model does not explicitly model the dentate gyrus (which is a significant research problem in itself), the learning rule used at least provides a principled reason for its existence.

Further evidence for this neuroanatomical mapping can be obtained from the connectivity between hippocampus and EC\@.
The superficial EC provides input to hippocampus, but does not receive direct input for hippocampus (TODO ref).
In contrast to that, the deep layers of EC receive input from hippocampus and might be relevant for recall, especially the recall of pre-experimental contex.
This is consistent with the connectivity in the model where the context network projects to the association matrix learning network attributed to hippocampus.
The learning network for $\mft$ also projects back to an ensemble recalling the prior context before it gets combined in a different ensemble.


TODO did they actually say anything about context-to-item matrix?

more going on in MTL, eg place cells


\section{Advances in large-scale cognitive modeling}
The primary objective of building the CUE model lead to a number of advances in large-scale cognitive modeling in general that are worth summarizing, even though not all have been presented as part of this thesis.
Most importantly, optimizations for high-dimensional representations in neural networks (\cite{gosmann216}; \cref{sec:hdrep}).
These allow to use fewer neurons which ultimately allows to build more complex networks without prohibitively long simulation times.
A similar benefit has an improved product network \parencite{gosmann2015-1} as the calculation of products is often required, for example for the implementation of the circular convolution.
Also a significant improvement to simulation times was achieved by the implementation of an optimization procedure in the Nengo simulator \parencite{gosmann2017}.
\Cref{sec:spa} described a new method for binding Semantic Pointers, the vector-derived transformation binding.
Even though it has not been used in the CUE model yet, it might provide a more precise binding and unbinding.
Finally, the independent accumulator network described in \cref{sec:ia} might not only be useful in recall, but for other large-scale models too.


%\begin{itemize}
    %\item firing rates!
    %\item basal ganglia involvement?
    %\item Control is the hard part
    %\item exact implementation of experimental protocol
    %\item fuzzy temp memory sensitive to noise and thus no alternative for context signal
%\end{itemize}
